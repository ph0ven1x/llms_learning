{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b887e3",
   "metadata": {},
   "source": [
    "# åŸºäºlangchainåˆ›å»ºè‡ªå·±ä¸“å±çš„å¯¹è¯å¤§æ¨¡å‹\n",
    "\n",
    "1. é¢†åŸŸç²¾å‡†å›ç­”\n",
    "\n",
    "2. æ•°æ®æ›´æ–°é¢‘ç¹\n",
    "\n",
    "3. ç”Ÿæˆå†…å®¹å¯è§£é‡Šè¿½æº¯\n",
    "\n",
    "4. æ•°æ®éšç§ä¿æŠ¤\n",
    "\n",
    "é€šè¿‡è¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å°†åŸºäº`LangChain`, `OpenAI(LLM)`,`vector DB` æ„å»ºä¸€ä¸ªå±äºè‡ªå·±çš„LLMæ¨¡å‹\n",
    "\n",
    "ä¸»è¦ä½¿ç”¨æŠ€æœ¯â€”â€”â€”â€”â€”â€”__*Retrieval Augmented Generation*__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc58f230",
   "metadata": {},
   "source": [
    "## ç¯å¢ƒå‡†å¤‡\n",
    "\n",
    "å¯ä»¥é€‰æ‹©å®˜æ–¹çš„openai_api_key\n",
    "\n",
    "ä¹Ÿå¯ä»¥é€‰æ‹©ä¸­é—´ä»£ç†å•†ï¼Œæ­¤æ—¶éœ€è¦openai_api_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa37d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -qU \\\n",
    "    langchain==0.0.316 \\\n",
    "    openai==0.28.1  \\\n",
    "    tiktoken==0.12.0  \\\n",
    "    cohere \\\n",
    "    chromadb==0.4.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d13153",
   "metadata": {},
   "source": [
    "## åˆ›å»ºä¸€ä¸ªå¯¹è¯æ¨¡å‹(No RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db964ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from getpass import getpass\n",
    "\n",
    "OPENAI_API_KEY = \"***********\"\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_base=\"https://**********/api/v1\",\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    model=\"openai/gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9a0ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Knock knock.\"),\n",
    "    AIMessage(content=\"Who's there?\"),\n",
    "    HumanMessage(content=\"Orange\"),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8c44d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Orange who? \n",
      "\n",
      "Orange you glad I didn't say banana? ğŸ˜„\n",
      "\n",
      "\n",
      "\n",
      "Want to play again?\n"
     ]
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d288e797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orange you glad I'm here to assist you?\n"
     ]
    }
   ],
   "source": [
    "messages.append(res)\n",
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd68a6",
   "metadata": {},
   "source": [
    "### å¤„ç†LLMå­˜åœ¨çš„ç¼ºé™·\n",
    "\n",
    "1. å®¹æ˜“å‡ºç°å¹»è§‰\n",
    "\n",
    "2. ä¿¡æ¯æ»å\n",
    "\n",
    "3. ä¸“ä¸šé¢†åŸŸæ·±åº¦çŸ¥è¯†åŒ®ä¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2819e13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æŠ±æ­‰ï¼Œæˆ‘ä¸ç†Ÿæ‚‰\"baichuan2\"æ¨¡å‹ã€‚è¯·é—®ä½ èƒ½æä¾›æ›´å¤šèƒŒæ™¯ä¿¡æ¯æˆ–è€…ä¸Šä¸‹æ–‡å—ï¼Ÿè¿™æ ·æˆ‘æ‰èƒ½æ›´å¥½åœ°å¸®åŠ©ä½ ã€‚\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åŠ©æ‰‹ã€‚\"),\n",
    "    HumanMessage(content=\"ä½ çŸ¥é“baichuan2æ¨¡å‹å—ï¼Ÿ\"),\n",
    "]\n",
    "\n",
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d02b2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baichuan 2æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡å¤šè¯­è¨€è¯­è¨€æ¨¡å‹ï¼Œå®ƒä¸“æ³¨äºè®­ç»ƒåœ¨å¤šç§è¯­è¨€ä¸­è¡¨ç°ä¼˜å¼‚çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸ä»…é™äºè‹±æ–‡ã€‚è¿™ä½¿å¾—Baichuan 2åœ¨å¤„ç†å„ç§è¯­è¨€çš„ä»»åŠ¡æ—¶èƒ½å¤Ÿå–å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚\n",
      "Baichuan 2æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒçš„ï¼Œä½¿ç”¨äº†åŒ…æ‹¬äº†2.6ä¸‡äº¿ä¸ªæ ‡è®°çš„åºå¤§è®­ç»ƒæ•°æ®é›†ã€‚ç›¸å¯¹äºä»¥å¾€çš„æ¨¡å‹ï¼ŒBaichuan 2æä¾›äº†æ›´ä¸°å¯Œçš„æ•°æ®èµ„æºï¼Œä»è€Œèƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒå¤šè¯­è¨€çš„å¼€å‘å’Œåº”ç”¨ã€‚\n",
      "Baichuan 2ä¸ä»…åœ¨é€šç”¨ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¿˜åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»å­¦å’Œæ³•å¾‹ï¼‰çš„ä»»åŠ¡ä¸­å±•ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚è¿™ä¸ºç‰¹å®šé¢†åŸŸçš„åº”ç”¨æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚\n"
     ]
    }
   ],
   "source": [
    "baichuan2_information = [\n",
    "    \"Baichuan 2æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡å¤šè¯­è¨€è¯­è¨€æ¨¡å‹ï¼Œå®ƒä¸“æ³¨äºè®­ç»ƒåœ¨å¤šç§è¯­è¨€ä¸­è¡¨ç°ä¼˜å¼‚çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸ä»…é™äºè‹±æ–‡ã€‚è¿™ä½¿å¾—Baichuan 2åœ¨å¤„ç†å„ç§è¯­è¨€çš„ä»»åŠ¡æ—¶èƒ½å¤Ÿå–å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚\",\n",
    "    \"Baichuan 2æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒçš„ï¼Œä½¿ç”¨äº†åŒ…æ‹¬äº†2.6ä¸‡äº¿ä¸ªæ ‡è®°çš„åºå¤§è®­ç»ƒæ•°æ®é›†ã€‚ç›¸å¯¹äºä»¥å¾€çš„æ¨¡å‹ï¼ŒBaichuan 2æä¾›äº†æ›´ä¸°å¯Œçš„æ•°æ®èµ„æºï¼Œä»è€Œèƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒå¤šè¯­è¨€çš„å¼€å‘å’Œåº”ç”¨ã€‚\",\n",
    "    \"Baichuan 2ä¸ä»…åœ¨é€šç”¨ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¿˜åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»å­¦å’Œæ³•å¾‹ï¼‰çš„ä»»åŠ¡ä¸­å±•ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚è¿™ä¸ºç‰¹å®šé¢†åŸŸçš„åº”ç”¨æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(baichuan2_information)\n",
    "print(source_knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ed12f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ä½ çŸ¥é“baichuan2æ¨¡å‹å—ï¼Ÿ\"\n",
    "\n",
    "prompt_template = f\"\"\"åŸºäºä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜ï¼š\n",
    "\n",
    "å†…å®¹:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79e9f076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baichuan 2æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡å¤šè¯­è¨€è¯­è¨€æ¨¡å‹ï¼Œå®ƒä¸“æ³¨äºè®­ç»ƒåœ¨å¤šç§è¯­è¨€ä¸­è¡¨ç°ä¼˜å¼‚çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸ä»…é™äºè‹±æ–‡ã€‚è¿™ä½¿å¾—Baichuan 2åœ¨å¤„ç†å„ç§è¯­è¨€çš„ä»»åŠ¡æ—¶èƒ½å¤Ÿå–å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚Baichuan 2æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒçš„ï¼Œä½¿ç”¨äº†åŒ…æ‹¬äº†2.6ä¸‡äº¿ä¸ªæ ‡è®°çš„åºå¤§è®­ç»ƒæ•°æ®é›†ã€‚ç›¸å¯¹äºä»¥å¾€çš„æ¨¡å‹ï¼ŒBaichuan 2æä¾›äº†æ›´ä¸°å¯Œçš„æ•°æ®èµ„æºï¼Œä»è€Œèƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒå¤šè¯­è¨€çš„å¼€å‘å’Œåº”ç”¨ã€‚Baichuan 2ä¸ä»…åœ¨é€šç”¨ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¿˜åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»å­¦å’Œæ³•å¾‹ï¼‰çš„ä»»åŠ¡ä¸­å±•ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚è¿™ä¸ºç‰¹å®šé¢€åŸŸçš„åº”ç”¨æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=prompt_template\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48972d87",
   "metadata": {},
   "source": [
    "## åˆ›å»ºä¸€ä¸ªRAGå¯¹è¯æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1902cf",
   "metadata": {},
   "source": [
    "### 1. åŠ è½½æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6921dedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: pypdf in c:\\users\\ph0ven1x\\vsprojects\\llms-learn\\.venv\\lib\\site-packages (6.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45ffc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"2309.10305v2.pdf\")\n",
    "\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3667e977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Baichuan 2: Open Large-scale Language Models\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan\\nDian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai\\nGuosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji\\nJian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma\\nMang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun\\nTao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng\\nXiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang\\nYiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu\\nBaichuan Inc.\\nAbstract\\nLarge language models (LLMs) have\\ndemonstrated remarkable performance on\\na variety of natural language tasks based\\non just a few examples of natural language\\ninstructions, reducing the need for extensive\\nfeature engineering. However, most powerful\\nLLMs are closed-source or limited in their\\ncapability for languages other than English. In\\nthis technical report, we present Baichuan 2,\\na series of large-scale multilingual language\\nmodels containing 7 billion and 13 billion\\nparameters, trained from scratch, on 2.6 trillion\\ntokens. Baichuan 2 matches or outperforms\\nother open-source models of similar size on\\npublic benchmarks like MMLU, CMMLU,\\nGSM8K, and HumanEval. Furthermore,\\nBaichuan 2 excels in vertical domains such\\nas medicine and law. We will release all\\npre-training model checkpoints to benefit the\\nresearch community in better understanding\\nthe training dynamics of Baichuan 2.\\n1 Introduction\\nThe field of large language models has witnessed\\npromising and remarkable progress in recent years.\\nThe size of language models has grown from\\nmillions of parameters, such as ELMo (Peters\\net al., 2018), GPT-1 (Radford et al., 2018), to\\nbillions or even trillions of parameters such as GPT-\\n3 (Brown et al., 2020), PaLM (Chowdhery et al.,\\n2022; Anil et al., 2023) and Switch Transformers\\n(Fedus et al., 2022). This increase in scale has\\nled to significant improvements in the capabilities\\nof language models, enabling more human-like\\nfluency and the ability to perform a diverse range\\nof natural language tasks. With the introduction of\\nAuthors are listed alphabetically, correspondent:\\ndaniel@baichuan-inc.com.\\nChatGPT (OpenAI, 2022) from OpenAI, the power\\nof these models to generate human-like text has\\ncaptured widespread public attention. ChatGPT\\ndemonstrates strong language proficiency across\\na variety of domains, from conversing casually to\\nexplaining complex concepts. This breakthrough\\nhighlights the potential for large language models\\nto automate tasks involving natural language\\ngeneration and comprehension.\\nWhile there have been exciting breakthroughs\\nand applications of LLMs, most leading LLMs like\\nGPT-4 (OpenAI, 2023), PaLM-2 (Anil et al., 2023),\\nand Claude (Claude, 2023) remain closed-sourced.\\nDevelopers and researchers have limited access to\\nthe full model parameters, making it difficult for\\nthe community to deeply study or fine-tune these\\nsystems. More openness and transparency around\\nLLMs could accelerate research and responsible\\ndevelopment within this rapidly advancing field.\\nLLaMA (Touvron et al., 2023a), a series of large\\nlanguage models developed by Meta containing up\\nto 65 billion parameters, has significantly benefited\\nthe LLM research community by being fully open-\\nsourced. The open nature of LLaMA, along with\\nother open-source LLMs such as OPT (Zhang\\net al., 2022), Bloom (Scao et al., 2022), MPT\\n(MosaicML, 2023) and Falcon (Penedo et al.,\\n2023), enables researchers to freely access the\\nmodels for examination, experimentation, and\\nfurther development. This transparency and access\\ndistinguishes LLaMA from other proprietary\\nLLMs. By providing full access, the open-source\\nLLMs have accelerated research and advances in\\nthe field, leading to new models like Alpaca (Taori\\net al., 2023), Vicuna (Chiang et al., 2023), and', metadata={'source': '2309.10305v2.pdf', 'page': 0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40045742",
   "metadata": {},
   "source": [
    "### 2. çŸ¥è¯†åˆ‡ç‰‡ï¼Œå°†æ–‡æ¡£åˆ†å‰²æˆå‡åŒ€çš„å—ã€‚æ¯ä¸ªå—æ˜¯ä¸€æ®µåŸå§‹æ–‡æœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "192b1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1200a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aceaf9b",
   "metadata": {},
   "source": [
    "### 3. åˆ©ç”¨embeddingæ¨¡å‹å¯¹æ¯ä¸ªæ–‡æœ¬ç‰‡è¿›è¡Œå‘é‡åŒ–ï¼Œå¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59826a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embed_model = OpenAIEmbeddings(\n",
    "    openai_api_base=\"https://*********/api/v1\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model=\"openai/text-embedding-3-large\"\n",
    ")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs, \n",
    "    embedding=embed_model, \n",
    "    collection_name=\"openai_embed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a25b3",
   "metadata": {},
   "source": [
    "### 4. é€šè¿‡å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢å’Œé—®é¢˜æœ€ç›¸å…³çš„kä¸ªæ–‡æ¡£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "337fe707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='adequate training of each word embedding. We\\nhave taken both these aspects into account. We\\nhave expanded the vocabulary size from 64,000\\nin Baichuan 1 to 125,696, aiming to strike a\\nbalance between computational efficiency and\\nmodel performance.\\nTokenizer V ocab Size Compression Rate â†“\\nLLaMA 2 32,000 1.037\\nBloom 250,680 0.501\\nChatGLM 2 64,794 0.527\\nBaichuan 1 64,000 0.570\\nBaichuan 2 125,696 0.498\\nTable 2: The vocab size and text compression rate of', metadata={'page': 2, 'source': '2309.10305v2.pdf'}), Document(page_content='adequate training of each word embedding. We\\nhave taken both these aspects into account. We\\nhave expanded the vocabulary size from 64,000\\nin Baichuan 1 to 125,696, aiming to strike a\\nbalance between computational efficiency and\\nmodel performance.\\nTokenizer V ocab Size Compression Rate â†“\\nLLaMA 2 32,000 1.037\\nBloom 250,680 0.501\\nChatGLM 2 64,794 0.527\\nBaichuan 1 64,000 0.570\\nBaichuan 2 125,696 0.498\\nTable 2: The vocab size and text compression rate of', metadata={'page': 2, 'source': '2309.10305v2.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"How large is the baichuan2 vocabulary?\"\n",
    "result = vectorstore.similarity_search(query ,k = 2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6209539",
   "metadata": {},
   "source": [
    "### 5. åŸå§‹`query`ä¸æ£€ç´¢å¾—åˆ°çš„æ–‡æœ¬ç»„åˆèµ·æ¥è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ï¼Œå¾—åˆ°æœ€ç»ˆçš„å›ç­”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79c446b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query.\n",
      "\n",
      "  contexts:\n",
      "  adequate training of each word embedding. We\n",
      "have taken both these aspects into account. We\n",
      "have expanded the vocabulary size from 64,000\n",
      "in Baichuan 1 to 125,696, aiming to strike a\n",
      "balance between computational efficiency and\n",
      "model performance.\n",
      "Tokenizer V ocab Size Compression Rate â†“\n",
      "LLaMA 2 32,000 1.037\n",
      "Bloom 250,680 0.501\n",
      "ChatGLM 2 64,794 0.527\n",
      "Baichuan 1 64,000 0.570\n",
      "Baichuan 2 125,696 0.498\n",
      "Table 2: The vocab size and text compression rate of\n",
      "adequate training of each word embedding. We\n",
      "have taken both these aspects into account. We\n",
      "have expanded the vocabulary size from 64,000\n",
      "in Baichuan 1 to 125,696, aiming to strike a\n",
      "balance between computational efficiency and\n",
      "model performance.\n",
      "Tokenizer V ocab Size Compression Rate â†“\n",
      "LLaMA 2 32,000 1.037\n",
      "Bloom 250,680 0.501\n",
      "ChatGLM 2 64,794 0.527\n",
      "Baichuan 1 64,000 0.570\n",
      "Baichuan 2 125,696 0.498\n",
      "Table 2: The vocab size and text compression rate of\n",
      "languages, such as Chinese.\n",
      "In this technical report, we introduce Baichuan\n",
      "2, a series of large-scale multilingual language\n",
      "models. Baichuan 2 has two separate models,\n",
      "Baichuan 2-7B with 7 billion parameters and\n",
      "Baichuan 2-13B with 13 billion parameters. Both\n",
      "models were trained on 2.6 trillion tokens, which\n",
      "to our knowledge is the largest to date, more than\n",
      "double that of Baichuan 1 (Baichuan, 2023b,a).\n",
      "With such a massive amount of training data,\n",
      "\n",
      "  query: How large is the baichuan2 vocabulary?\n"
     ]
    }
   ],
   "source": [
    "def augment_prompt(query: str):\n",
    "  # è·å–top3çš„æ–‡æœ¬ç‰‡æ®µ\n",
    "  results = vectorstore.similarity_search(query, k=3)\n",
    "  source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "  # æ„å»ºprompt\n",
    "  augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "  contexts:\n",
    "  {source_knowledge}\n",
    "\n",
    "  query: {query}\"\"\"\n",
    "  return augmented_prompt\n",
    "\n",
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8589655d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baichuan 2 has a vocabulary size of 125,696 words.\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºprompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
