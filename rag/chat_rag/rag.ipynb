{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b887e3",
   "metadata": {},
   "source": [
    "# 基于langchain创建自己专属的对话大模型\n",
    "\n",
    "1. 领域精准回答\n",
    "\n",
    "2. 数据更新频繁\n",
    "\n",
    "3. 生成内容可解释追溯\n",
    "\n",
    "4. 数据隐私保护\n",
    "\n",
    "通过这个例子，我们将基于`LangChain`, `OpenAI(LLM)`,`vector DB` 构建一个属于自己的LLM模型\n",
    "\n",
    "主要使用技术——————__*Retrieval Augmented Generation*__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc58f230",
   "metadata": {},
   "source": [
    "## 环境准备\n",
    "\n",
    "可以选择官方的openai_api_key\n",
    "\n",
    "也可以选择中间代理商，此时需要openai_api_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa37d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU \\\n",
    "    langchain-openai\\\n",
    "    langchain-core\\\n",
    "    langchain-text-splitters \\\n",
    "    langchain-chroma\\\n",
    "    chromadb \\\n",
    "    langchain \\\n",
    "    openai \\\n",
    "    tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d13153",
   "metadata": {},
   "source": [
    "## 创建一个对话模型(No RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db964ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from getpass import getpass\n",
    "\n",
    "OPENAI_API_KEY = \"************\"\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_base=\"https://************/v1\",\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    model=\"openai/gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a0ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Knock knock.\"),\n",
    "    AIMessage(content=\"Who's there?\"),\n",
    "    HumanMessage(content=\"Orange\"),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c44d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orange who?\n"
     ]
    }
   ],
   "source": [
    "res = chat.invoke(messages) \n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d288e797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orange you glad I guessed correctly?\n"
     ]
    }
   ],
   "source": [
    "# 将历史对话作为历史记录\n",
    "messages.append(res)\n",
    "\n",
    "messages.append(HumanMessage(content=\"guess\"))\n",
    "\n",
    "res = chat.invoke(messages) \n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd68a6",
   "metadata": {},
   "source": [
    "### 处理LLM存在的缺陷\n",
    "\n",
    "1. 容易出现幻觉\n",
    "\n",
    "2. 信息滞后\n",
    "\n",
    "3. 专业领域深度知识匮乏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2819e13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baichuan2 模型可能是指百川2模型，但是我无法找到更多关于这个模型的具体信息。请提供更多背景或上下文信息，这样我可以更好地帮助你。如果 baichuan2 模型是一个特定的概念、产品或领域，请提供更多细节，我将尽力回答你的问题。\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"你是一个专业的知识助手。\"),\n",
    "    HumanMessage(content=\"你知道baichuan2模型吗？\"),\n",
    "]\n",
    "\n",
    "res = chat.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d02b2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baichuan 2是一个大规模多语言语言模型，它专注于训练在多种语言中表现优异的模型，包括不仅限于英文。这使得Baichuan 2在处理各种语言的任务时能够取得显著的性能提升。\n",
      "Baichuan 2是从头开始训练的，使用了包括了2.6万亿个标记的庞大训练数据集。相对于以往的模型，Baichuan 2提供了更丰富的数据资源，从而能够更好地支持多语言的开发和应用。\n",
      "Baichuan 2不仅在通用任务上表现出色，还在特定领域（如医学和法律）的任务中展现了卓越的性能。这为特定领域的应用提供了强有力的支持。\n"
     ]
    }
   ],
   "source": [
    "baichuan2_information = [\n",
    "    \"Baichuan 2是一个大规模多语言语言模型，它专注于训练在多种语言中表现优异的模型，包括不仅限于英文。这使得Baichuan 2在处理各种语言的任务时能够取得显著的性能提升。\",\n",
    "    \"Baichuan 2是从头开始训练的，使用了包括了2.6万亿个标记的庞大训练数据集。相对于以往的模型，Baichuan 2提供了更丰富的数据资源，从而能够更好地支持多语言的开发和应用。\",\n",
    "    \"Baichuan 2不仅在通用任务上表现出色，还在特定领域（如医学和法律）的任务中展现了卓越的性能。这为特定领域的应用提供了强有力的支持。\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(baichuan2_information)\n",
    "print(source_knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ed12f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"你知道baichuan2模型吗？\"\n",
    "\n",
    "prompt_template = f\"\"\"基于以下内容回答问题：\n",
    "\n",
    "内容:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79e9f076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是的，Baichuan 2是一个大规模多语言语言模型，专注于训练在多种语言中表现优异的模型。它是从头开始训练的，使用了包括2.6万亿个标记的庞大训练数据集。Baichuan 2相比以往的模型提供了更丰富的数据资源，因此能够更好地支持多语言的开发和应用，在通用任务和特定领域（如医学和法律）的任务中表现出色，为特定领域的应用提供强有力支持。\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=prompt_template\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat.invoke(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48972d87",
   "metadata": {},
   "source": [
    "## 创建一个RAG对话模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1902cf",
   "metadata": {},
   "source": [
    "### 1. 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6921dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU langchain-chroma pypdf langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45ffc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"2309.10305v2.pdf\")\n",
    "\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3667e977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-21T00:15:31+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-21T00:15:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '2309.10305v2.pdf', 'total_pages': 28, 'page': 0, 'page_label': '1'}, page_content='Baichuan 2: Open Large-scale Language Models\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan\\nDian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai\\nGuosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji\\nJian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma\\nMang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun\\nTao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng\\nXiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang\\nYiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu\\nBaichuan Inc.\\nAbstract\\nLarge language models (LLMs) have\\ndemonstrated remarkable performance on\\na variety of natural language tasks based\\non just a few examples of natural language\\ninstructions, reducing the need for extensive\\nfeature engineering. However, most powerful\\nLLMs are closed-source or limited in their\\ncapability for languages other than English. In\\nthis technical report, we present Baichuan 2,\\na series of large-scale multilingual language\\nmodels containing 7 billion and 13 billion\\nparameters, trained from scratch, on 2.6 trillion\\ntokens. Baichuan 2 matches or outperforms\\nother open-source models of similar size on\\npublic benchmarks like MMLU, CMMLU,\\nGSM8K, and HumanEval. Furthermore,\\nBaichuan 2 excels in vertical domains such\\nas medicine and law. We will release all\\npre-training model checkpoints to benefit the\\nresearch community in better understanding\\nthe training dynamics of Baichuan 2.\\n1 Introduction\\nThe field of large language models has witnessed\\npromising and remarkable progress in recent years.\\nThe size of language models has grown from\\nmillions of parameters, such as ELMo (Peters\\net al., 2018), GPT-1 (Radford et al., 2018), to\\nbillions or even trillions of parameters such as GPT-\\n3 (Brown et al., 2020), PaLM (Chowdhery et al.,\\n2022; Anil et al., 2023) and Switch Transformers\\n(Fedus et al., 2022). This increase in scale has\\nled to significant improvements in the capabilities\\nof language models, enabling more human-like\\nfluency and the ability to perform a diverse range\\nof natural language tasks. With the introduction of\\nAuthors are listed alphabetically, correspondent:\\ndaniel@baichuan-inc.com.\\nChatGPT (OpenAI, 2022) from OpenAI, the power\\nof these models to generate human-like text has\\ncaptured widespread public attention. ChatGPT\\ndemonstrates strong language proficiency across\\na variety of domains, from conversing casually to\\nexplaining complex concepts. This breakthrough\\nhighlights the potential for large language models\\nto automate tasks involving natural language\\ngeneration and comprehension.\\nWhile there have been exciting breakthroughs\\nand applications of LLMs, most leading LLMs like\\nGPT-4 (OpenAI, 2023), PaLM-2 (Anil et al., 2023),\\nand Claude (Claude, 2023) remain closed-sourced.\\nDevelopers and researchers have limited access to\\nthe full model parameters, making it difficult for\\nthe community to deeply study or fine-tune these\\nsystems. More openness and transparency around\\nLLMs could accelerate research and responsible\\ndevelopment within this rapidly advancing field.\\nLLaMA (Touvron et al., 2023a), a series of large\\nlanguage models developed by Meta containing up\\nto 65 billion parameters, has significantly benefited\\nthe LLM research community by being fully open-\\nsourced. The open nature of LLaMA, along with\\nother open-source LLMs such as OPT (Zhang\\net al., 2022), Bloom (Scao et al., 2022), MPT\\n(MosaicML, 2023) and Falcon (Penedo et al.,\\n2023), enables researchers to freely access the\\nmodels for examination, experimentation, and\\nfurther development. This transparency and access\\ndistinguishes LLaMA from other proprietary\\nLLMs. By providing full access, the open-source\\nLLMs have accelerated research and advances in\\nthe field, leading to new models like Alpaca (Taori\\net al., 2023), Vicuna (Chiang et al., 2023), and')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40045742",
   "metadata": {},
   "source": [
    "### 2. 知识切片，将文档分割成均匀的块。每个块是一段原始文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "192b1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1200a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aceaf9b",
   "metadata": {},
   "source": [
    "### 3. 利用embedding模型对每个文本片进行向量化，并存储到向量数据库中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59826a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embed_model = OpenAIEmbeddings(\n",
    "    openai_api_base=\"https://************/v1\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model=\"openai/text-embedding-3-small\"\n",
    ")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs, \n",
    "    embedding=embed_model, \n",
    "    collection_name=\"openai_embed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a25b3",
   "metadata": {},
   "source": [
    "### 4. 通过向量相似度检索和问题最相关的k个文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "337fe707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='ae1301cd-f41f-4619-b2a9-45c995b66cb2', metadata={'author': '', 'trapped': '/False', 'keywords': '', 'page_label': '2', 'title': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '2309.10305v2.pdf', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'subject': '', 'total_pages': 28, 'page': 1, 'creationdate': '2023-09-21T00:15:31+00:00', 'moddate': '2023-09-21T00:15:31+00:00'}, page_content='languages, such as Chinese.\\nIn this technical report, we introduce Baichuan\\n2, a series of large-scale multilingual language\\nmodels. Baichuan 2 has two separate models,\\nBaichuan 2-7B with 7 billion parameters and\\nBaichuan 2-13B with 13 billion parameters. Both\\nmodels were trained on 2.6 trillion tokens, which\\nto our knowledge is the largest to date, more than\\ndouble that of Baichuan 1 (Baichuan, 2023b,a).\\nWith such a massive amount of training data,'), Document(id='f9f8491b-90e2-4ba5-a840-184864d0ef2b', metadata={'subject': '', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-21T00:15:31+00:00', 'trapped': '/False', 'source': '2309.10305v2.pdf', 'keywords': '', 'moddate': '2023-09-21T00:15:31+00:00', 'producer': 'pdfTeX-1.40.25', 'total_pages': 28, 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': '', 'author': '', 'page': 2, 'page_label': '3'}, page_content='adequate training of each word embedding. We\\nhave taken both these aspects into account. We\\nhave expanded the vocabulary size from 64,000\\nin Baichuan 1 to 125,696, aiming to strike a\\nbalance between computational efficiency and\\nmodel performance.\\nTokenizer V ocab Size Compression Rate ↓\\nLLaMA 2 32,000 1.037\\nBloom 250,680 0.501\\nChatGLM 2 64,794 0.527\\nBaichuan 1 64,000 0.570\\nBaichuan 2 125,696 0.498\\nTable 2: The vocab size and text compression rate of')]\n"
     ]
    }
   ],
   "source": [
    "query = \"How large is the baichuan2 vocabulary?\"\n",
    "result = vectorstore.similarity_search(query ,k = 2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6209539",
   "metadata": {},
   "source": [
    "### 5. 原始`query`与检索得到的文本组合起来输入到语言模型，得到最终的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79c446b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query.\n",
      "\n",
      "  contexts:\n",
      "  languages, such as Chinese.\n",
      "In this technical report, we introduce Baichuan\n",
      "2, a series of large-scale multilingual language\n",
      "models. Baichuan 2 has two separate models,\n",
      "Baichuan 2-7B with 7 billion parameters and\n",
      "Baichuan 2-13B with 13 billion parameters. Both\n",
      "models were trained on 2.6 trillion tokens, which\n",
      "to our knowledge is the largest to date, more than\n",
      "double that of Baichuan 1 (Baichuan, 2023b,a).\n",
      "With such a massive amount of training data,\n",
      "adequate training of each word embedding. We\n",
      "have taken both these aspects into account. We\n",
      "have expanded the vocabulary size from 64,000\n",
      "in Baichuan 1 to 125,696, aiming to strike a\n",
      "balance between computational efficiency and\n",
      "model performance.\n",
      "Tokenizer V ocab Size Compression Rate ↓\n",
      "LLaMA 2 32,000 1.037\n",
      "Bloom 250,680 0.501\n",
      "ChatGLM 2 64,794 0.527\n",
      "Baichuan 1 64,000 0.570\n",
      "Baichuan 2 125,696 0.498\n",
      "Table 2: The vocab size and text compression rate of\n",
      "Baichuan 2: Open Large-scale Language Models\n",
      "Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan\n",
      "Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai\n",
      "Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji\n",
      "Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma\n",
      "Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun\n",
      "\n",
      "  query: How large is the baichuan2 vocabulary?\n"
     ]
    }
   ],
   "source": [
    "def augment_prompt(query: str):\n",
    "  # 获取top3的文本片段\n",
    "  results = vectorstore.similarity_search(query, k=3)\n",
    "  source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "  # 构建prompt\n",
    "  augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "  contexts:\n",
    "  {source_knowledge}\n",
    "\n",
    "  query: {query}\"\"\"\n",
    "  return augmented_prompt\n",
    "\n",
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8589655d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baichuan 2 has a vocabulary size of 125,696 words, as mentioned in the technical report.\n"
     ]
    }
   ],
   "source": [
    "# 创建prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat.invoke(messages)\n",
    "\n",
    "print(res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-chat-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
