{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ffdc40",
   "metadata": {},
   "source": [
    "# 常用 pdf loader\n",
    "\n",
    "https://docs.langchain.com/oss/python/integrations/document_loaders#pdfs\n",
    "\n",
    "- PyPDF\n",
    "- PyMuPDF\n",
    "- PDFPlumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f296f064",
   "metadata": {},
   "source": [
    "## PyPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41165d6",
   "metadata": {},
   "source": [
    "### 提取文本\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c344269",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU langchain_community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa9d4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-Eval MMLU CMMLU Gaokao AGIEval BBH GSM8K HumanEval\n",
      "GPT-4 68.40 83.93 70.33 66.15 63.27 75.12 89.99 69.51\n",
      "GPT-3.5 Turbo 51.10 68.54 54.06 47.07 46.13 61.59 57.77 52.44\n",
      "LLaMA-7B 27.10 35.10 26.75 27.81 28.17 32.38 9.78 11.59\n",
      "LLaMA 2-7B 28.90 45.73 31.38 25.97 26.53 39.16 16.22 12.80\n",
      "MPT-7B 27.15 27.93 26.00 26.54 24.83 35.20 8.64 14.02\n",
      "Falcon-7B 24.23 26.03 25.66 24.24 24.10 28.77 5.46 -\n",
      "ChatGLM 2-6B (base)∗ 51.70 47.86 - - - 33.68 32.37 -\n",
      "Baichuan 1-7B 42.80 42.30 44.02 36.34 34.44 32.48 9.17 9.20\n",
      "7B\n",
      "Baichuan 2-7B-Base 54.00 54.16 57.07 47.47 42.73 41.56 24.49 18.29\n",
      "LLaMA-13B 28.50 46.30 31.15 28.23 28.22 37.89 20.55 15.24\n",
      "LLaMA 2-13B 35.80 55.09 37.99 30.83 32.29 46.98 28.89 15.24\n",
      "Vicuna-13B 32.80 52.00 36.28 30.11 31.55 43.04 28.13 16.46\n",
      "Chinese-Alpaca-Plus-13B 38.80 43.90 33.43 34.78 35.46 28.94 11.98 16.46\n",
      "XVERSE-13B 53.70 55.21 58.44 44.69 42.54 38.06 18.20 15.85\n",
      "Baichuan 1-13B-Base 52.40 51.60 55.30 49.69 43.20 43.01 26.76 11.59\n",
      "13B\n",
      "Baichuan 2-13B-Base 58.10 59.17 61.97 54.33 48.17 48.78 52.77 17.07\n",
      "Table 1: Overall results of Baichuan 2 compared with other similarly sized LLMs on general benchmarks. * denotes\n",
      "results derived from official websites.\n",
      "Figure 1: The distribution of different categories of\n",
      "Baichuan 2 training data.\n",
      "Data processing: For data processing, we focus\n",
      "on data frequency and quality. Data frequency\n",
      "relies on clustering and deduplication. We built\n",
      "a large-scale deduplication and clustering system\n",
      "supporting both LSH-like features and dense\n",
      "embedding features. This system can cluster\n",
      "and deduplicate trillion-scale data within hours.\n",
      "Based on the clustering, individual documents,\n",
      "paragraphs, and sentences are deduplicated and\n",
      "scored. Those scores are then used for data\n",
      "sampling in pre-training. The size of the training\n",
      "data at different stages of data processing is shown\n",
      "in Figure 2.\n",
      "2.2 Architecture\n",
      "The model architecture of Baichuan 2 is based on\n",
      "the prevailing Transformer (Vaswani et al., 2017).\n",
      "Nevertheless, we made several modifications which\n",
      "we detailed below.\n",
      "2.3 Tokenizer\n",
      "A tokenizer needs to balance two critical factors:\n",
      "a high compression rate for efficient inference,\n",
      "and an appropriately sized vocabulary to ensure\n",
      "adequate training of each word embedding. We\n",
      "have taken both these aspects into account. We\n",
      "have expanded the vocabulary size from 64,000\n",
      "in Baichuan 1 to 125,696, aiming to strike a\n",
      "balance between computational efficiency and\n",
      "model performance.\n",
      "Tokenizer V ocab Size Compression Rate ↓\n",
      "LLaMA 2 32,000 1.037\n",
      "Bloom 250,680 0.501\n",
      "ChatGLM 2 64,794 0.527\n",
      "Baichuan 1 64,000 0.570\n",
      "Baichuan 2 125,696 0.498\n",
      "Table 2: The vocab size and text compression rate of\n",
      "Baichuan 2’s tokenizer compared with other models.\n",
      "The lower the better.\n",
      "We use byte-pair encoding (BPE) (Shibata et al.,\n",
      "1999) from SentencePiece (Kudo and Richardson,\n",
      "2018) to tokenize the data. Specifically, we do not\n",
      "apply any normalization to the input text and we\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\n",
    "    file_path=\"baichuan2.pdf\",\n",
    "    mode=\"page\",\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "137b9807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction\\nThe field of large language models has witnessed\\npromising and remarkable progress in recent years.\\nThe size of language models has grown from\\nmillions of parameters, such as ELMo (Peters\\net al., 2018), GPT-1 (Radford et al., 2018), to\\nbillions or even trillions of parameters such as GPT-\\n3 (Brown et al., 2020), PaLM (Chowdhery et al.,\\n2022; Anil et al., 2023) and Switch Transformers\\n(Fedus et al., 20'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "docs[0].page_content[1583:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb497e6",
   "metadata": {},
   "source": [
    "### 提取图片文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c13e98",
   "metadata": {},
   "source": [
    "需要注意的是：提取文本时需要修改源码：`langchain_community\\document_loaders\\parsers\\pdf.py\\PyPDFParser\\extract_images_from_page`方法中存在一个错误，该错误导致在保存任何图像数据之前检查图像像素image_bytes是否为空。结果，所有图像图像都被跳过，没有将图像传送给 OCR 解析器。\n",
    "\n",
    "当前环境\n",
    "```text\n",
    "langchain-community      0.4.1\n",
    "rapidocr-onnxruntime     1.4.4\n",
    "numpy                    2.2.6\n",
    "```\n",
    "\n",
    "\n",
    "修复方式：将`Image.fromarray(np_image).save(image_bytes, format=\"PNG\")`移动到`if image_bytes.getbuffer().nbytes == 0:`前\n",
    "\n",
    "原代码：\n",
    "```python\n",
    "                if np_image is not None:\n",
    "                    image_bytes = io.BytesIO()\n",
    "\n",
    "                    if image_bytes.getbuffer().nbytes == 0:\n",
    "                        continue\n",
    "\n",
    "                    \n",
    "                    blob = Blob.from_data(image_bytes.getvalue(), mime_type=\"image/png\")\n",
    "                    image_text = next(self.images_parser.lazy_parse(blob)).page_content\n",
    "                    images.append(\n",
    "                        _format_inner_image(blob, image_text, self.images_inner_format)\n",
    "                    )\n",
    "```\n",
    "\n",
    "修复后代码：\n",
    "\n",
    "```python\n",
    "                if np_image is not None:\n",
    "                    image_bytes = io.BytesIO()\n",
    "                    Image.fromarray(np_image).save(image_bytes, format=\"PNG\")\n",
    "                    if image_bytes.getbuffer().nbytes == 0:\n",
    "                        continue\n",
    "\n",
    "                    \n",
    "                    blob = Blob.from_data(image_bytes.getvalue(), mime_type=\"image/png\")\n",
    "                    image_text = next(self.images_parser.lazy_parse(blob)).page_content\n",
    "                    images.append(\n",
    "                        _format_inner_image(blob, image_text, self.images_inner_format)\n",
    "                    )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335c9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU rapidocr-onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb89d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-Eval MMLU CMMLU Gaokao AGIEval BBH GSM8K HumanEval\n",
      "GPT-4 68.40 83.93 70.33 66.15 63.27 75.12 89.99 69.51\n",
      "GPT-3.5 Turbo 51.10 68.54 54.06 47.07 46.13 61.59 57.77 52.44\n",
      "LLaMA-7B 27.10 35.10 26.75 27.81 28.17 32.38 9.78 11.59\n",
      "LLaMA 2-7B 28.90 45.73 31.38 25.97 26.53 39.16 16.22 12.80\n",
      "MPT-7B 27.15 27.93 26.00 26.54 24.83 35.20 8.64 14.02\n",
      "Falcon-7B 24.23 26.03 25.66 24.24 24.10 28.77 5.46 -\n",
      "ChatGLM 2-6B (base)∗ 51.70 47.86 - - - 33.68 32.37 -\n",
      "Baichuan 1-7B 42.80 42.30 44.02 36.34 34.44 32.48 9.17 9.20\n",
      "7B\n",
      "Baichuan 2-7B-Base 54.00 54.16 57.07 47.47 42.73 41.56 24.49 18.29\n",
      "LLaMA-13B 28.50 46.30 31.15 28.23 28.22 37.89 20.55 15.24\n",
      "LLaMA 2-13B 35.80 55.09 37.99 30.83 32.29 46.98 28.89 15.24\n",
      "Vicuna-13B 32.80 52.00 36.28 30.11 31.55 43.04 28.13 16.46\n",
      "Chinese-Alpaca-Plus-13B 38.80 43.90 33.43 34.78 35.46 28.94 11.98 16.46\n",
      "XVERSE-13B 53.70 55.21 58.44 44.69 42.54 38.06 18.20 15.85\n",
      "Baichuan 1-13B-Base 52.40 51.60 55.30 49.69 43.20 43.01 26.76 11.59\n",
      "13B\n",
      "Baichuan 2-13B-Base 58.10 59.17 61.97 54.33 48.17 48.78 52.77 17.07\n",
      "Table 1: Overall results of Baichuan 2 compared with other similarly sized LLMs on general benchmarks. * denotes\n",
      "results derived from official websites.\n",
      "Figure 1: The distribution of different categories of\n",
      "Baichuan 2 training data.\n",
      "Data processing: For data processing, we focus\n",
      "on data frequency and quality. Data frequency\n",
      "relies on clustering and deduplication. We built\n",
      "a large-scale deduplication and clustering system\n",
      "supporting both LSH-like features and dense\n",
      "embedding features. This system can cluster\n",
      "and deduplicate trillion-scale data within hours.\n",
      "Based on the clustering, individual documents,\n",
      "paragraphs, and sentences are deduplicated and\n",
      "scored. Those scores are then used for data\n",
      "sampling in pre-training. The size of the training\n",
      "data at different stages of data processing is shown\n",
      "in Figure 2.\n",
      "2.2 Architecture\n",
      "The model architecture of Baichuan 2 is based on\n",
      "the prevailing Transformer (Vaswani et al., 2017).\n",
      "Nevertheless, we made several modifications which\n",
      "we detailed below.\n",
      "2.3 Tokenizer\n",
      "A tokenizer needs to balance two critical factors:\n",
      "a high compression rate for efficient inference,\n",
      "and an appropriately sized vocabulary to ensure\n",
      "adequate training of each word embedding. We\n",
      "have taken both these aspects into account. We\n",
      "have expanded the vocabulary size from 64,000\n",
      "in Baichuan 1 to 125,696, aiming to strike a\n",
      "balance between computational efficiency and\n",
      "model performance.\n",
      "Tokenizer V ocab Size Compression Rate ↓\n",
      "LLaMA 2 32,000 1.037\n",
      "Bloom 250,680 0.501\n",
      "ChatGLM 2 64,794 0.527\n",
      "Baichuan 1 64,000 0.570\n",
      "Baichuan 2 125,696 0.498\n",
      "Table 2: The vocab size and text compression rate of\n",
      "Baichuan 2’s tokenizer compared with other models.\n",
      "The lower the better.\n",
      "We use byte-pair encoding (BPE) (Shibata et al.,\n",
      "1999) from SentencePiece (Kudo and Richardson,\n",
      "2018) to tokenize the data. Specifically, we do not\n",
      "apply any normalization to the input text and we\n",
      "\n",
      "\n",
      "\n",
      "Academic disciplines\n",
      "Technology\n",
      "Philosophy\n",
      "Business\n",
      "Information\n",
      "Energy\n",
      "Human behavior\n",
      "12%\n",
      "Entertainment\n",
      "Health\n",
      "9%\n",
      "Society\n",
      "Education\n",
      "Humanities\n",
      "Culture\n",
      "6%\n",
      "Code\n",
      "Time\n",
      "3%\n",
      "Sports\n",
      "Mass media\n",
      "Engineering\n",
      "0%\n",
      "Geography\n",
      "History\n",
      "Nature\n",
      "Knowledge\n",
      "Law\n",
      "Politics\n",
      "People\n",
      "Mathematics\n",
      "Internet\n",
      "Language\n",
      "Military\n",
      "Science\n",
      "Religion\n",
      "Government\n",
      "Economy\n",
      "Communication\n",
      "Food and drink\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders.parsers import RapidOCRBlobParser\n",
    "\n",
    "loader = PyPDFLoader(\n",
    "    file_path=\"baichuan2.pdf\",\n",
    "    mode=\"page\",\n",
    "    images_inner_format=\"markdwon-img\",\n",
    "    images_parser=RapidOCRBlobParser()\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68c8ad",
   "metadata": {},
   "source": [
    "## PyMuPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42627bea",
   "metadata": {},
   "source": [
    "### 提取文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a31517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU langchain_community pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c726759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='C-Eval MMLU CMMLU Gaokao AGIEval BBH\n",
      "GSM8K HumanEval\n",
      "GPT-4\n",
      "68.40\n",
      "83.93\n",
      "70.33\n",
      "66.15\n",
      "63.27\n",
      "75.12\n",
      "89.99\n",
      "69.51\n",
      "GPT-3.5 Turbo\n",
      "51.10\n",
      "68.54\n",
      "54.06\n",
      "47.07\n",
      "46.13\n",
      "61.59\n",
      "57.77\n",
      "52.44\n",
      "LLaMA-7B\n",
      "27.10\n",
      "35.10\n",
      "26.75\n",
      "27.81\n",
      "28.17\n",
      "32.38\n",
      "9.78\n",
      "11.59\n",
      "LLaMA 2-7B\n",
      "28.90\n",
      "45.73\n",
      "31.38\n",
      "25.97\n",
      "26.53\n",
      "39.16\n",
      "16.22\n",
      "12.80\n",
      "MPT-7B\n",
      "27.15\n",
      "27.93\n",
      "26.00\n",
      "26.54\n",
      "24.83\n",
      "35.20\n",
      "8.64\n",
      "14.02\n",
      "Falcon-7B\n",
      "24.23\n",
      "26.03\n",
      "25.66\n",
      "24.24\n",
      "24.10\n",
      "28.77\n",
      "5.46\n",
      "-\n",
      "ChatGLM 2-6B (base)∗\n",
      "51.70\n",
      "47.86\n",
      "-\n",
      "-\n",
      "-\n",
      "33.68\n",
      "32.37\n",
      "-\n",
      "Baichuan 1-7B\n",
      "42.80\n",
      "42.30\n",
      "44.02\n",
      "36.34\n",
      "34.44\n",
      "32.48\n",
      "9.17\n",
      "9.20\n",
      "7B\n",
      "Baichuan 2-7B-Base\n",
      "54.00\n",
      "54.16\n",
      "57.07\n",
      "47.47\n",
      "42.73\n",
      "41.56\n",
      "24.49\n",
      "18.29\n",
      "LLaMA-13B\n",
      "28.50\n",
      "46.30\n",
      "31.15\n",
      "28.23\n",
      "28.22\n",
      "37.89\n",
      "20.55\n",
      "15.24\n",
      "LLaMA 2-13B\n",
      "35.80\n",
      "55.09\n",
      "37.99\n",
      "30.83\n",
      "32.29\n",
      "46.98\n",
      "28.89\n",
      "15.24\n",
      "Vicuna-13B\n",
      "32.80\n",
      "52.00\n",
      "36.28\n",
      "30.11\n",
      "31.55\n",
      "43.04\n",
      "28.13\n",
      "16.46\n",
      "Chinese-Alpaca-Plus-13B\n",
      "38.80\n",
      "43.90\n",
      "33.43\n",
      "34.78\n",
      "35.46\n",
      "28.94\n",
      "11.98\n",
      "16.46\n",
      "XVERSE-13B\n",
      "53.70\n",
      "55.21\n",
      "58.44\n",
      "44.69\n",
      "42.54\n",
      "38.06\n",
      "18.20\n",
      "15.85\n",
      "Baichuan 1-13B-Base\n",
      "52.40\n",
      "51.60\n",
      "55.30\n",
      "49.69\n",
      "43.20\n",
      "43.01\n",
      "26.76\n",
      "11.59\n",
      "13B\n",
      "Baichuan 2-13B-Base\n",
      "58.10\n",
      "59.17\n",
      "61.97\n",
      "54.33\n",
      "48.17\n",
      "48.78\n",
      "52.77\n",
      "17.07\n",
      "Table 1: Overall results of Baichuan 2 compared with other similarly sized LLMs on general benchmarks. * denotes\n",
      "results derived from official websites.\n",
      "Figure 1: The distribution of different categories of\n",
      "Baichuan 2 training data.\n",
      "Data processing: For data processing, we focus\n",
      "on data frequency and quality. Data frequency\n",
      "relies on clustering and deduplication. We built\n",
      "a large-scale deduplication and clustering system\n",
      "supporting both LSH-like features and dense\n",
      "embedding features.\n",
      "This system can cluster\n",
      "and deduplicate trillion-scale data within hours.\n",
      "Based on the clustering, individual documents,\n",
      "paragraphs, and sentences are deduplicated and\n",
      "scored.\n",
      "Those scores are then used for data\n",
      "sampling in pre-training. The size of the training\n",
      "data at different stages of data processing is shown\n",
      "in Figure 2.\n",
      "2.2\n",
      "Architecture\n",
      "The model architecture of Baichuan 2 is based on\n",
      "the prevailing Transformer (Vaswani et al., 2017).\n",
      "Nevertheless, we made several modifications which\n",
      "we detailed below.\n",
      "2.3\n",
      "Tokenizer\n",
      "A tokenizer needs to balance two critical factors:\n",
      "a high compression rate for efficient inference,\n",
      "and an appropriately sized vocabulary to ensure\n",
      "adequate training of each word embedding. We\n",
      "have taken both these aspects into account. We\n",
      "have expanded the vocabulary size from 64,000\n",
      "in Baichuan 1 to 125,696, aiming to strike a\n",
      "balance between computational efficiency and\n",
      "model performance.\n",
      "Tokenizer\n",
      "Vocab Size\n",
      "Compression Rate ↓\n",
      "LLaMA 2\n",
      "32,000\n",
      "1.037\n",
      "Bloom\n",
      "250,680\n",
      "0.501\n",
      "ChatGLM 2\n",
      "64,794\n",
      "0.527\n",
      "Baichuan 1\n",
      "64,000\n",
      "0.570\n",
      "Baichuan 2\n",
      "125,696\n",
      "0.498\n",
      "Table 2: The vocab size and text compression rate of\n",
      "Baichuan 2’s tokenizer compared with other models.\n",
      "The lower the better.\n",
      "We use byte-pair encoding (BPE) (Shibata et al.,\n",
      "1999) from SentencePiece (Kudo and Richardson,\n",
      "2018) to tokenize the data. Specifically, we do not\n",
      "apply any normalization to the input text and we' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-21T00:15:31+00:00', 'source': 'baichuan2.pdf', 'file_path': 'baichuan2.pdf', 'total_pages': 28, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-09-21T00:15:31+00:00', 'trapped': '', 'modDate': 'D:20230921001531Z', 'creationDate': 'D:20230921001531Z', 'page': 2}\n",
      "C-Eval MMLU CMMLU Gaokao AGIEval BBH\n",
      "GSM8K HumanEval\n",
      "GPT-4\n",
      "68.40\n",
      "83.93\n",
      "70.33\n",
      "66.15\n",
      "63.27\n",
      "75.12\n",
      "89.99\n",
      "69.51\n",
      "GPT-3.5 Turbo\n",
      "51.10\n",
      "68.54\n",
      "54.06\n",
      "47.07\n",
      "46.13\n",
      "61.59\n",
      "57.77\n",
      "52.44\n",
      "LLaMA-7B\n",
      "27.10\n",
      "35.10\n",
      "26.75\n",
      "27.81\n",
      "28.17\n",
      "32.38\n",
      "9.78\n",
      "11.59\n",
      "LLaMA 2-7B\n",
      "28.90\n",
      "45.73\n",
      "31.38\n",
      "25.97\n",
      "26.53\n",
      "39.16\n",
      "16.22\n",
      "12.80\n",
      "MPT-7B\n",
      "27.15\n",
      "27.93\n",
      "26.00\n",
      "26.54\n",
      "24.83\n",
      "35.20\n",
      "8.64\n",
      "14.02\n",
      "Falcon-7B\n",
      "24.23\n",
      "26.03\n",
      "25.66\n",
      "24.24\n",
      "24.10\n",
      "28.77\n",
      "5.46\n",
      "-\n",
      "ChatGLM 2-6B (base)∗\n",
      "51.70\n",
      "47.86\n",
      "-\n",
      "-\n",
      "-\n",
      "33.68\n",
      "32.37\n",
      "-\n",
      "Baichuan 1-7B\n",
      "42.80\n",
      "42.30\n",
      "44.02\n",
      "36.34\n",
      "34.44\n",
      "32.48\n",
      "9.17\n",
      "9.20\n",
      "7B\n",
      "Baichuan 2-7B-Base\n",
      "54.00\n",
      "54.16\n",
      "57.07\n",
      "47.47\n",
      "42.73\n",
      "41.56\n",
      "24.49\n",
      "18.29\n",
      "LLaMA-13B\n",
      "28.50\n",
      "46.30\n",
      "31.15\n",
      "28.23\n",
      "28.22\n",
      "37.89\n",
      "20.55\n",
      "15.24\n",
      "LLaMA 2-13B\n",
      "35.80\n",
      "55.09\n",
      "37.99\n",
      "30.83\n",
      "32.29\n",
      "46.98\n",
      "28.89\n",
      "15.24\n",
      "Vicuna-13B\n",
      "32.80\n",
      "52.00\n",
      "36.28\n",
      "30.11\n",
      "31.55\n",
      "43.04\n",
      "28.13\n",
      "16.46\n",
      "Chinese-Alpaca-Plus-13B\n",
      "38.80\n",
      "43.90\n",
      "33.43\n",
      "34.78\n",
      "35.46\n",
      "28.94\n",
      "11.98\n",
      "16.46\n",
      "XVERSE-13B\n",
      "53.70\n",
      "55.21\n",
      "58.44\n",
      "44.69\n",
      "42.54\n",
      "38.06\n",
      "18.20\n",
      "15.85\n",
      "Baichuan 1-13B-Base\n",
      "52.40\n",
      "51.60\n",
      "55.30\n",
      "49.69\n",
      "43.20\n",
      "43.01\n",
      "26.76\n",
      "11.59\n",
      "13B\n",
      "Baichuan 2-13B-Base\n",
      "58.10\n",
      "59.17\n",
      "61.97\n",
      "54.33\n",
      "48.17\n",
      "48.78\n",
      "52.77\n",
      "17.07\n",
      "Table 1: Overall results of Baichuan 2 compared with other similarly sized LLMs on general benchmarks. * denotes\n",
      "results derived from official websites.\n",
      "Figure 1: The distribution of different categories of\n",
      "Baichuan 2 training data.\n",
      "Data processing: For data processing, we focus\n",
      "on data frequency and quality. Data frequency\n",
      "relies on clustering and deduplication. We built\n",
      "a large-scale deduplication and clustering system\n",
      "supporting both LSH-like features and dense\n",
      "embedding features.\n",
      "This system can cluster\n",
      "and deduplicate trillion-scale data within hours.\n",
      "Based on the clustering, individual documents,\n",
      "paragraphs, and sentences are deduplicated and\n",
      "scored.\n",
      "Those scores are then used for data\n",
      "sampling in pre-training. The size of the training\n",
      "data at different stages of data processing is shown\n",
      "in Figure 2.\n",
      "2.2\n",
      "Architecture\n",
      "The model architecture of Baichuan 2 is based on\n",
      "the prevailing Transformer (Vaswani et al., 2017).\n",
      "Nevertheless, we made several modifications which\n",
      "we detailed below.\n",
      "2.3\n",
      "Tokenizer\n",
      "A tokenizer needs to balance two critical factors:\n",
      "a high compression rate for efficient inference,\n",
      "and an appropriately sized vocabulary to ensure\n",
      "adequate training of each word embedding. We\n",
      "have taken both these aspects into account. We\n",
      "have expanded the vocabulary size from 64,000\n",
      "in Baichuan 1 to 125,696, aiming to strike a\n",
      "balance between computational efficiency and\n",
      "model performance.\n",
      "Tokenizer\n",
      "Vocab Size\n",
      "Compression Rate ↓\n",
      "LLaMA 2\n",
      "32,000\n",
      "1.037\n",
      "Bloom\n",
      "250,680\n",
      "0.501\n",
      "ChatGLM 2\n",
      "64,794\n",
      "0.527\n",
      "Baichuan 1\n",
      "64,000\n",
      "0.570\n",
      "Baichuan 2\n",
      "125,696\n",
      "0.498\n",
      "Table 2: The vocab size and text compression rate of\n",
      "Baichuan 2’s tokenizer compared with other models.\n",
      "The lower the better.\n",
      "We use byte-pair encoding (BPE) (Shibata et al.,\n",
      "1999) from SentencePiece (Kudo and Richardson,\n",
      "2018) to tokenize the data. Specifically, we do not\n",
      "apply any normalization to the input text and we\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\n",
    "    file_path=\"baichuan2.pdf\",\n",
    "    mode=\"page\",\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dd8c6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction\\nThe field of large language models has witnessed\\npromising and remarkable progress in recent years.\\nThe size of language models has grown from\\nmillions of parameters, such as ELMo (Peters\\net al., 2018), GPT-1 (Radford et al., 2018), to\\nbillions or even trillions of parameters such as GPT-\\n3 (Brown et al., 2020), PaLM (Chowdhery et al.,\\n2022; Anil et al., 2023) and Switch Transformers\\n(Fedus et al., 20'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[1583:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951bbe21",
   "metadata": {},
   "source": [
    "可以发现表格处理是不如PyPDF的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99527ec3",
   "metadata": {},
   "source": [
    "### 提取图片文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f11524",
   "metadata": {},
   "source": [
    "同样PyMuPDF也存在问题，也需要修改源码：\n",
    "\n",
    "修改方式与PyPDF类似：\n",
    "\n",
    "修改`langchain_community\\document_loaders\\parsers\\pdf.py\\PyMuPDFParser\\_extract_images_from_page`方法：\n",
    "\n",
    "将`numpy.save(image_bytes, image)`放到`if image_bytes.getbuffer().nbytes == 0:`前\n",
    "\n",
    "\n",
    "修改前：\n",
    "\n",
    "```python\n",
    "            if self.images_parser:\n",
    "                xref = img[0]\n",
    "                pix = pymupdf.Pixmap(doc, xref)\n",
    "                image = np.frombuffer(pix.samples, dtype=np.uint8).reshape(\n",
    "                    pix.height, pix.width, -1\n",
    "                )\n",
    "                image_bytes = io.BytesIO()\n",
    "                \n",
    "                if image_bytes.getbuffer().nbytes == 0:\n",
    "                    continue\n",
    "\n",
    "                numpy.save(image_bytes, image)\n",
    "                blob = Blob.from_data(\n",
    "                    image_bytes.getvalue(), mime_type=\"application/x-npy\"\n",
    "                )\n",
    "                image_text = next(self.images_parser.lazy_parse(blob)).page_content\n",
    "\n",
    "                images.append(\n",
    "                    _format_inner_image(blob, image_text, self.images_inner_format)\n",
    "                )\n",
    "```\n",
    "\n",
    "修改后：\n",
    "\n",
    "```python\n",
    "            if self.images_parser:\n",
    "                xref = img[0]\n",
    "                pix = pymupdf.Pixmap(doc, xref)\n",
    "                image = np.frombuffer(pix.samples, dtype=np.uint8).reshape(\n",
    "                    pix.height, pix.width, -1\n",
    "                )\n",
    "                image_bytes = io.BytesIO()\n",
    "                numpy.save(image_bytes, image)\n",
    "                if image_bytes.getbuffer().nbytes == 0:\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                blob = Blob.from_data(\n",
    "                    image_bytes.getvalue(), mime_type=\"application/x-npy\"\n",
    "                )\n",
    "                image_text = next(self.images_parser.lazy_parse(blob)).page_content\n",
    "\n",
    "                images.append(\n",
    "                    _format_inner_image(blob, image_text, self.images_inner_format)\n",
    "                )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2b6aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU rapidocr-onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7cc6bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-Eval MMLU CMMLU Gaokao AGIEval BBH\n",
      "GSM8K HumanEval\n",
      "GPT-4\n",
      "68.40\n",
      "83.93\n",
      "70.33\n",
      "66.15\n",
      "63.27\n",
      "75.12\n",
      "89.99\n",
      "69.51\n",
      "GPT-3.5 Turbo\n",
      "51.10\n",
      "68.54\n",
      "54.06\n",
      "47.07\n",
      "46.13\n",
      "61.59\n",
      "57.77\n",
      "52.44\n",
      "LLaMA-7B\n",
      "27.10\n",
      "35.10\n",
      "26.75\n",
      "27.81\n",
      "28.17\n",
      "32.38\n",
      "9.78\n",
      "11.59\n",
      "LLaMA 2-7B\n",
      "28.90\n",
      "45.73\n",
      "31.38\n",
      "25.97\n",
      "26.53\n",
      "39.16\n",
      "16.22\n",
      "12.80\n",
      "MPT-7B\n",
      "27.15\n",
      "27.93\n",
      "26.00\n",
      "26.54\n",
      "24.83\n",
      "35.20\n",
      "8.64\n",
      "14.02\n",
      "Falcon-7B\n",
      "24.23\n",
      "26.03\n",
      "25.66\n",
      "24.24\n",
      "24.10\n",
      "28.77\n",
      "5.46\n",
      "-\n",
      "ChatGLM 2-6B (base)∗\n",
      "51.70\n",
      "47.86\n",
      "-\n",
      "-\n",
      "-\n",
      "33.68\n",
      "32.37\n",
      "-\n",
      "Baichuan 1-7B\n",
      "42.80\n",
      "42.30\n",
      "44.02\n",
      "36.34\n",
      "34.44\n",
      "32.48\n",
      "9.17\n",
      "9.20\n",
      "7B\n",
      "Baichuan 2-7B-Base\n",
      "54.00\n",
      "54.16\n",
      "57.07\n",
      "47.47\n",
      "42.73\n",
      "41.56\n",
      "24.49\n",
      "18.29\n",
      "LLaMA-13B\n",
      "28.50\n",
      "46.30\n",
      "31.15\n",
      "28.23\n",
      "28.22\n",
      "37.89\n",
      "20.55\n",
      "15.24\n",
      "LLaMA 2-13B\n",
      "35.80\n",
      "55.09\n",
      "37.99\n",
      "30.83\n",
      "32.29\n",
      "46.98\n",
      "28.89\n",
      "15.24\n",
      "Vicuna-13B\n",
      "32.80\n",
      "52.00\n",
      "36.28\n",
      "30.11\n",
      "31.55\n",
      "43.04\n",
      "28.13\n",
      "16.46\n",
      "Chinese-Alpaca-Plus-13B\n",
      "38.80\n",
      "43.90\n",
      "33.43\n",
      "34.78\n",
      "35.46\n",
      "28.94\n",
      "11.98\n",
      "16.46\n",
      "XVERSE-13B\n",
      "53.70\n",
      "55.21\n",
      "58.44\n",
      "44.69\n",
      "42.54\n",
      "38.06\n",
      "18.20\n",
      "15.85\n",
      "Baichuan 1-13B-Base\n",
      "52.40\n",
      "51.60\n",
      "55.30\n",
      "49.69\n",
      "43.20\n",
      "43.01\n",
      "26.76\n",
      "11.59\n",
      "13B\n",
      "Baichuan 2-13B-Base\n",
      "58.10\n",
      "59.17\n",
      "61.97\n",
      "54.33\n",
      "48.17\n",
      "48.78\n",
      "52.77\n",
      "17.07\n",
      "Table 1: Overall results of Baichuan 2 compared with other similarly sized LLMs on general benchmarks. * denotes\n",
      "results derived from official websites.\n",
      "Figure 1: The distribution of different categories of\n",
      "Baichuan 2 training data.\n",
      "Data processing: For data processing, we focus\n",
      "on data frequency and quality. Data frequency\n",
      "relies on clustering and deduplication. We built\n",
      "a large-scale deduplication and clustering system\n",
      "supporting both LSH-like features and dense\n",
      "embedding features.\n",
      "This system can cluster\n",
      "and deduplicate trillion-scale data within hours.\n",
      "Based on the clustering, individual documents,\n",
      "paragraphs, and sentences are deduplicated and\n",
      "scored.\n",
      "Those scores are then used for data\n",
      "sampling in pre-training. The size of the training\n",
      "data at different stages of data processing is shown\n",
      "in Figure 2.\n",
      "2.2\n",
      "Architecture\n",
      "The model architecture of Baichuan 2 is based on\n",
      "the prevailing Transformer (Vaswani et al., 2017).\n",
      "Nevertheless, we made several modifications which\n",
      "we detailed below.\n",
      "2.3\n",
      "Tokenizer\n",
      "A tokenizer needs to balance two critical factors:\n",
      "a high compression rate for efficient inference,\n",
      "and an appropriately sized vocabulary to ensure\n",
      "adequate training of each word embedding. We\n",
      "have taken both these aspects into account. We\n",
      "have expanded the vocabulary size from 64,000\n",
      "in Baichuan 1 to 125,696, aiming to strike a\n",
      "balance between computational efficiency and\n",
      "model performance.\n",
      "Tokenizer\n",
      "Vocab Size\n",
      "Compression Rate ↓\n",
      "LLaMA 2\n",
      "32,000\n",
      "1.037\n",
      "Bloom\n",
      "250,680\n",
      "0.501\n",
      "ChatGLM 2\n",
      "64,794\n",
      "0.527\n",
      "Baichuan 1\n",
      "64,000\n",
      "0.570\n",
      "Baichuan 2\n",
      "125,696\n",
      "0.498\n",
      "Table 2: The vocab size and text compression rate of\n",
      "Baichuan 2’s tokenizer compared with other models.\n",
      "The lower the better.\n",
      "We use byte-pair encoding (BPE) (Shibata et al.,\n",
      "1999) from SentencePiece (Kudo and Richardson,\n",
      "2018) to tokenize the data. Specifically, we do not\n",
      "apply any normalization to the input text and we\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "![Academic disciplines\n",
      "Technology\n",
      "Philosophy\n",
      "Business\n",
      "Information\n",
      "Energy\n",
      "Human behavior\n",
      "12%\n",
      "Entertainment\n",
      "Health\n",
      "9%\n",
      "Society\n",
      "Education\n",
      "Humanities\n",
      "Culture\n",
      "6%\n",
      "Code\n",
      "Time\n",
      "3%\n",
      "Sports\n",
      "Mass media\n",
      "Engineering\n",
      "0%\n",
      "Geography\n",
      "History\n",
      "Nature\n",
      "Knowledge\n",
      "Law\n",
      "Politics\n",
      "People\n",
      "Mathematics\n",
      "Internet\n",
      "Language\n",
      "Military\n",
      "Science\n",
      "Religion\n",
      "Government\n",
      "Economy\n",
      "Communication\n",
      "Food and drink](#)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders.parsers import RapidOCRBlobParser\n",
    "\n",
    "loader = PyMuPDFLoader(\n",
    "    file_path=\"baichuan2.pdf\",\n",
    "    mode=\"page\",\n",
    "    images_inner_format=\"markdown-img\",\n",
    "    images_parser=RapidOCRBlobParser(),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9426d70a",
   "metadata": {},
   "source": [
    "## PDFPlumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30fbb6",
   "metadata": {},
   "source": [
    "### 提取文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a685a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU langchain-community pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d509c5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-Eval MMLU CMMLU Gaokao AGIEval BBH GSM8K HumanEval\n",
      "GPT-4 68.40 83.93 70.33 66.15 63.27 75.12 89.99 69.51\n",
      "GPT-3.5Turbo 51.10 68.54 54.06 47.07 46.13 61.59 57.77 52.44\n",
      "LLaMA-7B 27.10 35.10 26.75 27.81 28.17 32.38 9.78 11.59\n",
      "LLaMA2-7B 28.90 45.73 31.38 25.97 26.53 39.16 16.22 12.80\n",
      "MPT-7B 27.15 27.93 26.00 26.54 24.83 35.20 8.64 14.02\n",
      "7B Falcon-7B 24.23 26.03 25.66 24.24 24.10 28.77 5.46 -\n",
      "ChatGLM2-6B(base)∗ 51.70 47.86 - - - 33.68 32.37 -\n",
      "Baichuan1-7B 42.80 42.30 44.02 36.34 34.44 32.48 9.17 9.20\n",
      "Baichuan2-7B-Base 54.00 54.16 57.07 47.47 42.73 41.56 24.49 18.29\n",
      "LLaMA-13B 28.50 46.30 31.15 28.23 28.22 37.89 20.55 15.24\n",
      "LLaMA2-13B 35.80 55.09 37.99 30.83 32.29 46.98 28.89 15.24\n",
      "Vicuna-13B 32.80 52.00 36.28 30.11 31.55 43.04 28.13 16.46\n",
      "13B Chinese-Alpaca-Plus-13B 38.80 43.90 33.43 34.78 35.46 28.94 11.98 16.46\n",
      "XVERSE-13B 53.70 55.21 58.44 44.69 42.54 38.06 18.20 15.85\n",
      "Baichuan1-13B-Base 52.40 51.60 55.30 49.69 43.20 43.01 26.76 11.59\n",
      "Baichuan2-13B-Base 58.10 59.17 61.97 54.33 48.17 48.78 52.77 17.07\n",
      "Table1: OverallresultsofBaichuan2comparedwithothersimilarlysizedLLMsongeneralbenchmarks. *denotes\n",
      "resultsderivedfromofficialwebsites.\n",
      "2.2 Architecture\n",
      "ThemodelarchitectureofBaichuan2isbasedon\n",
      "theprevailingTransformer(Vaswanietal.,2017).\n",
      "Nevertheless,wemadeseveralmodificationswhich\n",
      "wedetailedbelow.\n",
      "2.3 Tokenizer\n",
      "A tokenizer needs to balance two critical factors:\n",
      "a high compression rate for efficient inference,\n",
      "and an appropriately sized vocabulary to ensure\n",
      "adequate training of each word embedding. We\n",
      "have taken both these aspects into account. We\n",
      "have expanded the vocabulary size from 64,000\n",
      "in Baichuan 1 to 125,696, aiming to strike a\n",
      "balance between computational efficiency and\n",
      "modelperformance.\n",
      "Figure 1: The distribution of different categories of\n",
      "Baichuan2trainingdata.\n",
      "Tokenizer VocabSize CompressionRate↓\n",
      "LLaMA2 32,000 1.037\n",
      "Data processing: For data processing, we focus\n",
      "Bloom 250,680 0.501\n",
      "on data frequency and quality. Data frequency\n",
      "ChatGLM2 64,794 0.527\n",
      "relies on clustering and deduplication. We built\n",
      "Baichuan1 64,000 0.570\n",
      "alarge-scalededuplicationandclusteringsystem\n",
      "Baichuan2 125,696 0.498\n",
      "supporting both LSH-like features and dense\n",
      "embedding features. This system can cluster\n",
      "Table2: Thevocabsizeandtextcompressionrateof\n",
      "and deduplicate trillion-scale data within hours. Baichuan 2’s tokenizer compared with other models.\n",
      "Based on the clustering, individual documents, Thelowerthebetter.\n",
      "paragraphs, and sentences are deduplicated and\n",
      "scored. Those scores are then used for data Weusebyte-pairencoding(BPE)(Shibataetal.,\n",
      "samplinginpre-training. Thesizeofthetraining 1999)fromSentencePiece(KudoandRichardson,\n",
      "dataatdifferentstagesofdataprocessingisshown 2018)totokenizethedata. Specifically,wedonot\n",
      "inFigure2. apply any normalization to the input text and we\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "loader = PDFPlumberLoader(\"baichuan2.pdf\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[2].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a55099",
   "metadata": {},
   "source": [
    "PDFPlumber的表格处理非常出众"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e5c49ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction\\nThe field of large language models has witnessed\\npromising and remarkable progress in recent years.\\nThe size of language models has grown from\\nmillions of parameters, such as ELMo (Peters\\net al., 2018), GPT-1 (Radford et al., 2018), to\\nbillions or even trillions of parameters such as GPT-\\n3 (Brown et al., 2020), PaLM (Chowdhery et al.,\\n2022; Anil et al., 2023) and Switch Transformers\\n(Fedus et al., 20'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[1583:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff600a9",
   "metadata": {},
   "source": [
    "### 提取图片文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a00ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU rapidocr-onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8384d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-Eval MMLU CMMLU Gaokao AGIEval BBH GSM8K HumanEval\n",
      "GPT-4 68.40 83.93 70.33 66.15 63.27 75.12 89.99 69.51\n",
      "GPT-3.5Turbo 51.10 68.54 54.06 47.07 46.13 61.59 57.77 52.44\n",
      "LLaMA-7B 27.10 35.10 26.75 27.81 28.17 32.38 9.78 11.59\n",
      "LLaMA2-7B 28.90 45.73 31.38 25.97 26.53 39.16 16.22 12.80\n",
      "MPT-7B 27.15 27.93 26.00 26.54 24.83 35.20 8.64 14.02\n",
      "7B Falcon-7B 24.23 26.03 25.66 24.24 24.10 28.77 5.46 -\n",
      "ChatGLM2-6B(base)∗ 51.70 47.86 - - - 33.68 32.37 -\n",
      "Baichuan1-7B 42.80 42.30 44.02 36.34 34.44 32.48 9.17 9.20\n",
      "Baichuan2-7B-Base 54.00 54.16 57.07 47.47 42.73 41.56 24.49 18.29\n",
      "LLaMA-13B 28.50 46.30 31.15 28.23 28.22 37.89 20.55 15.24\n",
      "LLaMA2-13B 35.80 55.09 37.99 30.83 32.29 46.98 28.89 15.24\n",
      "Vicuna-13B 32.80 52.00 36.28 30.11 31.55 43.04 28.13 16.46\n",
      "13B Chinese-Alpaca-Plus-13B 38.80 43.90 33.43 34.78 35.46 28.94 11.98 16.46\n",
      "XVERSE-13B 53.70 55.21 58.44 44.69 42.54 38.06 18.20 15.85\n",
      "Baichuan1-13B-Base 52.40 51.60 55.30 49.69 43.20 43.01 26.76 11.59\n",
      "Baichuan2-13B-Base 58.10 59.17 61.97 54.33 48.17 48.78 52.77 17.07\n",
      "Table1: OverallresultsofBaichuan2comparedwithothersimilarlysizedLLMsongeneralbenchmarks. *denotes\n",
      "resultsderivedfromofficialwebsites.\n",
      "2.2 Architecture\n",
      "ThemodelarchitectureofBaichuan2isbasedon\n",
      "theprevailingTransformer(Vaswanietal.,2017).\n",
      "Nevertheless,wemadeseveralmodificationswhich\n",
      "wedetailedbelow.\n",
      "2.3 Tokenizer\n",
      "A tokenizer needs to balance two critical factors:\n",
      "a high compression rate for efficient inference,\n",
      "and an appropriately sized vocabulary to ensure\n",
      "adequate training of each word embedding. We\n",
      "have taken both these aspects into account. We\n",
      "have expanded the vocabulary size from 64,000\n",
      "in Baichuan 1 to 125,696, aiming to strike a\n",
      "balance between computational efficiency and\n",
      "modelperformance.\n",
      "Figure 1: The distribution of different categories of\n",
      "Baichuan2trainingdata.\n",
      "Tokenizer VocabSize CompressionRate↓\n",
      "LLaMA2 32,000 1.037\n",
      "Data processing: For data processing, we focus\n",
      "Bloom 250,680 0.501\n",
      "on data frequency and quality. Data frequency\n",
      "ChatGLM2 64,794 0.527\n",
      "relies on clustering and deduplication. We built\n",
      "Baichuan1 64,000 0.570\n",
      "alarge-scalededuplicationandclusteringsystem\n",
      "Baichuan2 125,696 0.498\n",
      "supporting both LSH-like features and dense\n",
      "embedding features. This system can cluster\n",
      "Table2: Thevocabsizeandtextcompressionrateof\n",
      "and deduplicate trillion-scale data within hours. Baichuan 2’s tokenizer compared with other models.\n",
      "Based on the clustering, individual documents, Thelowerthebetter.\n",
      "paragraphs, and sentences are deduplicated and\n",
      "scored. Those scores are then used for data Weusebyte-pairencoding(BPE)(Shibataetal.,\n",
      "samplinginpre-training. Thesizeofthetraining 1999)fromSentencePiece(KudoandRichardson,\n",
      "dataatdifferentstagesofdataprocessingisshown 2018)totokenizethedata. Specifically,wedonot\n",
      "inFigure2. apply any normalization to the input text and we\n",
      "Academic disciplines\n",
      "Technology\n",
      "Philosophy\n",
      "Business\n",
      "Information\n",
      "Energy\n",
      "Human behavior\n",
      "12%\n",
      "Entertainment\n",
      "Health\n",
      "9%\n",
      "Society\n",
      "Education\n",
      "Humanities\n",
      "Culture\n",
      "6%\n",
      "Code\n",
      "Time\n",
      "3%\n",
      "Sports\n",
      "Mass media\n",
      "0%\n",
      "Engineering\n",
      "Geography\n",
      "History\n",
      "Nature\n",
      "Knowledge\n",
      "Law\n",
      "Politics\n",
      "People\n",
      "Mathematics\n",
      "Internet\n",
      "Language\n",
      "Military\n",
      "Science\n",
      "Religion\n",
      "Government\n",
      "Economy\n",
      "Communication\n",
      "Food and drink\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "loader = PDFPlumberLoader(\"baichuan2.pdf\", extract_images=True)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[2].page_content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf_loader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
